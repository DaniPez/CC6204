{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tarea_1_CC6204_2018",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1-7Alg0DzdesaVPVuDghwoXbN0pa8ds1t",
          "timestamp": 1521245040538
        }
      ]
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "4zhjpqvcdo5o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Tarea 1 <br/> CC6204 Deep Learning, Universidad de Chile \n",
        "\n",
        "En esta tarea **progamarás a mano** varios aspectos de redes neuronales Feed Forward. La idea es familiarizarse con tensores, funciones de activación, derivadas, el algoritmo de backpropagation, algoritmos de optimización, regularización, entrenamiento, y búsqueda de hiperparámetros. No se espera obtener excelentes resultados en problemas de clasificación reales, si no más bien aplicar los conceptos teóricos aprendidos en clases y así entenderlos de manera más precisa. La tarea tiene varias partes (algunas opcionales) y es **considerablemente larga** así que se recomienda empezar lo antes posible desarrollándola. \n",
        "\n",
        "Tal vez sea razonable/provechoso hacer una primera versión de la tarea desde la Parte 1 hasta la Parte 5, sin considerar todos los aspectos y detalles. Por ejemplo, podrías programar una red neuronal solo con activaciones sigmoid y tangente hiperbólica (que veremos o ya las vimos en clases), y una cantidad fija de capas (incluso una sola capa escondida) para darse cuenta de que se entienden los conceptos básicos y que la red efectivamente aprende algo. Luego de esto volver y comenzar toda la tarea otra vez, ahora desarollando todos los detalles.\n",
        "\n",
        "Se sugiere seguir las siguientes fechas para no atrasarse: \n",
        "*   Partes 1, 2: viernes 23 de marzo ([Hoja de respuestas](https://colab.research.google.com/drive/1V3vHkKRh_2XIMxDtH-LoQzmR3DW0X421))\n",
        "*   Partes 3, 4, 5, 8 (inicio): viernes 6 de abril\n",
        "*   Partes 6, 7, 8 (completa): viernes 20 de abril\n",
        "\n",
        "\n",
        "### IMPORTANTE 1: A menos que se exprese lo contrario, sólo podrás utilizar las clases y funciones en el módulo [`torch`](http://pytorch.org/docs/0.3.1/torch.html).\n",
        "\n",
        "### IMPORTANTE 2: Hay algunas partes de la tarea que aun están en construcción (están indicadas).\n",
        "\n",
        "(por Jorge Pérez, https://github.com/jorgeperezrojas, [@perez](https://twitter.com/perez))\n"
      ]
    },
    {
      "metadata": {
        "id": "I_kVL7undMBx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Este notebook está pensado para correr en CoLaboratory. \n",
        "# Comenzamos instalando las librerías necesarias.\n",
        "# Si lo estás ejecutando localmente posiblemente no sea necesario\n",
        "# reinstalar todo.\n",
        "\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n",
        "!pip install -q ipdb\n",
        "\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2xbEy3Php0a2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Parte 1: Funciones de activación, derivadas y función de salida\n",
        "\n",
        "En esta parte programarás varias funciones que serán de utilidad cuando construyas tu red neuronal. Además tendrás que derivar a mano. Una cosa **muy importante en esta y las siguientes partes**: evita los loops (`for`, `while`, etc.) a toda costa! todo lo que se pueda hacer con operaciones de tensores sin iterar será muy eficiente (en CPU y GPU)."
      ]
    },
    {
      "metadata": {
        "id": "p3nKwdzpqF2y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1a) Funciones de activación\n",
        "\n",
        "En esta parte debes programar las siguientes funciones de activación:\n",
        "\n",
        "*   `relu`, que para cada valor $x$ en un tensor computa el máximo entre $0$ y $x$,  \n",
        "*   `swish`, propuesta en el artículo [Searching for Activation Functions](https://arxiv.org/abs/1710.05941), y\n",
        "*   `celu`, propuesta en el artículo [Continuously Differentiable Exponential Linear Units](https://arxiv.org/abs/1704.07483).\n",
        "\n",
        "En cada caso tu función debe recibir un tensor y entregar un tensor con la función aplicada a todos sus elementos. **Importante**:  tanto `swish` como `celu` tienen un parámetro que puede modificarse durante el entrenamiento de una red que utilice estas funciones de activación por lo que para estas funciones además del tensor debes recibir el parámetro correspondiente. "
      ]
    },
    {
      "metadata": {
        "id": "_EhdkSIOFvR9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Como ejemplo, estas son implementaciones de las funciones `sig` y `tanh`."
      ]
    },
    {
      "metadata": {
        "id": "p80-9lwaUAix",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def sig(T):\n",
        "  return torch.reciprocal(1 + torch.exp(-1 * T))\n",
        "\n",
        "def tanh(T):\n",
        "  E = torch.exp(T)\n",
        "  e = torch.exp(-1 * T)\n",
        "  return (E - e) * torch.reciprocal(E + e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4VtQWFePbtrF",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Tu código acá\n",
        "\n",
        "def relu(T):\n",
        "  pass\n",
        "\n",
        "def swish(T, ):\n",
        "  pass\n",
        "\n",
        "def celu(T, ):\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ph78UHMwdwp2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1b) Derivando las funciones de activación\n",
        "\n",
        "Calcula a mano las derivadas de las funciones `relu`, `swish` y `celu`. Recuerda que `swish` y `celu` tienen ambas parámetros adicionales así que debes calcular las derivadas (parciales) con respecto a ellos también. Intenta expresar las derivadas en términos de aplicaciones de la misma función (o sub expresiones de esta). Por ejemplo para la función `sigmoid` tenemos que: \n",
        "<br><br>\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial\\ \\text{sigmoid}(x)}{\\partial x}\\; =\\; \\text{sigmoid}(x)\\big(1 - \\text{sigmoid}(x)\\big)\n",
        "\\end{equation}\n",
        "<br><br>\n",
        "\n",
        "Las deducciones correspondientes puedes entregarlas en otro archivo, pero debes escribir tus respuestas finales acá."
      ]
    },
    {
      "metadata": {
        "id": "SawUSepjenJt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial\\ \\text{relu}(x)}{\\partial x} = \\ldots \\\\\n",
        "\\end{equation}\n",
        "<br><br>\n",
        "\n",
        "\\begin{eqnarray}\n",
        "\\frac{\\partial\\ \\text{swish}(x, \\ldots)}{\\partial x} & = & \\ldots \\\\\n",
        "\\frac{\\partial\\ \\text{swish}(x, \\ldots)}{\\partial \\ldots} & = & \\ldots \\\\\n",
        "\\end{eqnarray}\n",
        "<br><br>\n",
        "\n",
        "\\begin{eqnarray}\n",
        "\\frac{\\partial\\ \\text{celu}(x, \\ldots)}{\\partial x} & = & \\ldots \\\\\n",
        "\\frac{\\partial\\ \\text{celu}(x, \\ldots)}{\\partial \\ldots} & = & \\ldots \\\\\n",
        "\\end{eqnarray}"
      ]
    },
    {
      "metadata": {
        "id": "adagNIVPp0fF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1c) Softmax\n",
        "\n",
        "En esta parte debes programar la función `softmax`. Esta es una función tal que para una secuencia de valores $(x_1,\\ldots,x_n)$  el resultado de $\\text{softmax}(x_1,\\ldots,x_n)$ es otra secuencia $(s_1,\\ldots,s_n)$ tal que\n",
        "\\begin{equation}\n",
        "s_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n}e^{x_j}}\n",
        "\\end{equation}\n",
        "Para esto primero demuestra que si a cada elemento de $(x_1,\\ldots,x_n)$ se le resta el mismo valor, entonces el resultado de `softmax` no varía. Es decir que $\\text{softmax}(x_1-M,\\ldots,x_n-M)=\\text{softmax}(x_1,\\ldots,x_n)$. Usa este hecho para programar una versión de `softmax` que primero le resta a todos los elementos el máximo valor de la secuencia. Esta nueva versión debiera ser numéricamente más estable.\n",
        "\n",
        "Tu función debiera recibir un tensor y el resultado de `softmax` debiera calcularse sobre alguna dimensión del tensor (por defecto la dimensión `0`) de manera tal que `softmax` es aplicado para la secuencia de valores obtenidos fijando un índice para la dimensión elegida. Por ejemplo, si `softmax` recibe un tensor de dos dimensiones $T_{ij}$ y se elige la dimensión $0$, entonces se debe computar $\\text{softmax}(T_{i1},\\ldots,T_{in})$ para cada $i$.\n",
        "\n",
        "Nota que el resultado de `softmax` es un tensor de las mismas dimensiones de la entrada."
      ]
    },
    {
      "metadata": {
        "id": "kwEot28n3tD2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Puedes entregar la demostración en otro archivo o escribir tu respuesta acá..."
      ]
    },
    {
      "metadata": {
        "id": "5hVvsHp-dGk4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "cellView": "code"
      },
      "cell_type": "code",
      "source": [
        "# Tu código acá\n",
        "\n",
        "def softmax(T, dim=0, estable=True):\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2m3i_1ijPLr_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Parte 2: Red neuronal y pasada hacia adelante (forward)\n",
        "\n",
        "En esta parte empezaremos a programar nuestra red neuronal, en particular la pasada hacia adelante para una red que resolverá problemas de clasificación con varias clases. Supondremos que cada capa se verá de la forma\n",
        "\\begin{equation}\n",
        "h^{(\\ell)} = f^{(\\ell)}(h^{(\\ell-1)} W^{(\\ell)}+b^{(\\ell)})\n",
        "\\end{equation}\n",
        "y que la predicción final estará dada por\n",
        "\\begin{equation}\n",
        "\\text{softmax}(h^{(L)}U+c).\n",
        "\\end{equation}"
      ]
    },
    {
      "metadata": {
        "id": "E_1cvyD9PiMY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2a) Clase para red neuronal\n",
        "\n",
        "Programa una clase `FFNN` que en su inicializador reciba los siguientes parámetros:\n",
        "\n",
        "*   Cantidad de neuronas de la capa de entrada `F`\n",
        "*   Lista de cantidades de neuronas en cada capa escondida `l_h`\n",
        "*   Lista de funciones de activación `l_a`\n",
        "*   Cantidad de neuronas de la capa de salida `C` (`C` $\\geq 2$)\n",
        "\n",
        "El inicializador debería crear todos los parámetros para la red como tensores (`torch.tensor`) de las dimensiones correspondientes, y almacenar lo necesario para poder computar la pasada hacia adelante (siguiente parte). Inicializa los pesos con números aleatorios pequeños y los bias como $0$. No olvides almacenar los parámetros como parte de la clase para que otros métodos de la clase tengan acceso a ellos también. En particular, deja todos los parámetros en una lista `parametros` que luego puedas recorrer. \n",
        "\n",
        "Una llamada de ejemplo para crear un objeto de tu clase es:\n",
        "```\n",
        "red_neuronal = FFNN(300,[50,30],[relu,sig],10)\n",
        "``` \n",
        "lo que debiera crear todos los parámetros para una red con 300 neuronas en la capa de entrada, luego una capa escondida de 50 neuronas con activación relu, luego una capa con 30 neuronas y activación sigmoid y finalmente una capa de 10 neuronas de salida. <br><br>\n",
        "\n",
        "Puedes agregarle todos los parámetros opcionales que estimes conveniente. Uno recomendable, es alguna forma de pasarle los valores iniciales de los parámetros, lo que servirá por ejemplo para cargar redes pre-entrenadas, inicializar los valores de manera más efectiva, o para hacer debugging del código. También puedes pedir los valores iniciales de los parámetros adicionales de las funciones `celu` y `swish`."
      ]
    },
    {
      "metadata": {
        "id": "eW3fhiQPTMLT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Tu código debiera comenzar así (ojo que acá solo empieza el código, \n",
        "# lo iremos completando más adelante).\n",
        "\n",
        "class FFNN():\n",
        "  def __init__(self, F, l_h, l_a, C):\n",
        "    pass\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xbDabd2LMQCx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2b) Usando la GPU\n",
        "\n",
        "Agrega dos métodos a tu clase, el método `gpu()` y el método `cpu()`. El primer método debiera pasar todos los parámetros de la red a la GPU (usando, por ejemplo, el método `.cuda()` de la clase la clase `torch.Tensor`). El seguno método debiera pasar todos los parámetros de la red a la CPU (usando, por ejemplo, el método `.cpu()` de la clase la clase `torch.Tensor`). Comprueba que los parámetros efectivamente se pasan a la GPU mirando el espacio utlizado en ella."
      ]
    },
    {
      "metadata": {
        "id": "fTgxEjzjMPj6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Tu código debiera continuar así \n",
        "\n",
        "class FFNN():\n",
        "  def __init__(self, F, l_h, l_a, C):\n",
        "    pass\n",
        "  \n",
        "  def gpu(self):\n",
        "    # pasa todos los parámetros a la GPU\n",
        "    pass\n",
        "  \n",
        "  def cpu(self):\n",
        "    # pasa todos los parámetros a la CPU\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kUk5lUM8TtBL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2c) Pasada hacia adelante\n",
        "\n",
        "Programa la pasada hacia adelante de tu red neuronal en el método `forward` de la clase `FFNN`. La función debiera recibir un tensor de dimensiones `(B,F)` como entrada donde `B` es el tamaño del mini paquete de ejemplos pasados a tu red (y `F` la cantidad de *features* de los ejemplos). Para computar la pasada hacia adelante, tu red debiera usar los parámetros creados en el inicializador y las funciones de activación entregadas también en el inicializador. Al finalizar tu red debiera generar predicciones en la forma de probabilidades, aplicando la función `softmax`. El resultado del último `softmax` debiera ser un tensor de dimensiones `(B,C)` que es lo que la función debe retornar (donde `C` represent a la cantidad de clases a clasificar).\n",
        "\n",
        "Esta función puede requerir *cachear* algunas computaciones intermedias para usarlas más adelante, por lo que probablemente tendrás que volver y modificar esta función después de que desarrolles las siguientes partes de la tarea."
      ]
    },
    {
      "metadata": {
        "id": "QseFyNzlUuAx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Tu código debiera continuar así \n",
        "\n",
        "class FFNN():\n",
        "  def __init__(self, F, l_h, l_a, C):\n",
        "    pass\n",
        "  \n",
        "  def gpu(self):\n",
        "    pass\n",
        "  \n",
        "  def cpu(self):\n",
        "    pass\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # Usar parámetros y funciones de activación\n",
        "    # El valor de retorno debiera ser y = softmax(capa_de_salida)\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "871AjprcvVPC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2d) Probando tu red con un modelo pre-entrenado\n",
        "\n",
        "En esta parte usarás la pasada hacia adelante de tu red con parámetros de una red pre entrenada. La red fue entrenada para el problema \"Varita Mágica\" de la Olimpiada Escolar de Informática 2017 que se trata de predecir la clase de dibujos hechos en mapas de bits de 64 por 64, dentro de 10 posibilidades. Puedes ver una descripción del problema [acá](https://github.com/jorgeperezrojas/cc6204-DeepLearning-DCCUChile/tree/master/2018/tareas/tarea1/recursos/varita_magica). En ese mismo enlace encontrarás datos y un modelo de ejemplo. \n",
        "\n",
        "En la carpeta `modelos/ejemplo` encontrarás varios archivos de texto que representan los parámetros de una red con 2 capas escondidas ambas de 15 neuronas con activación $\\text{sigmoid}$. La capa de entrada por su parte es de 4096 neuronas y la de salida de 10 neuronas. La predicción utiliza la función $\\text{softmax}$. Los archivos están nombrados como `W1`, `b1`, `W2`, `b2`, `U` y `c` que representan, respectivamente, a $W^{(1)}$, $b^{(1)}$, $W^{(2)}$, $b^{(2)}$, $U$ y $c$ en la descripción de más arriba.\n",
        "\n",
        "Crea una red con las características anteriores usando tu clase FFNN y pruébala con los datos en las carpetas `data/test_set` y `data/train_set`. Para cargar los parámetros puedes usar la funcion `numpy.loadtxt`. Calcula el porcentaje de certeza de la red sobre el conjunto de todos los datos (debería ser cercana a 90%). Procura usar tensores para computar la predicción (no iteraciones!) y prueba hacer los cálculos en la GPU (para lo cual deberás pasar el tensor de inputs a la GPU también antes de la llamada a `forward`)."
      ]
    },
    {
      "metadata": {
        "id": "WdX6bv6NvTYR",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Tu código acá"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nTWFl38WD0s8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Usa dos ejemplos que la red haya clasificado incorrectamente y visualízalos usando [matplotlib](https://matplotlib.org/users/index.html)."
      ]
    },
    {
      "metadata": {
        "id": "_rYqUumoD0NL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Tu código acá"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D9SpFNK7FkJ9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Parte 3: Más derivadas y back propagation\n",
        "\n",
        "En esta parte comenzaremos a usar el algoritmo de back propagation para poder actualizar los parámetros de nuestra red neuronal. <br> \n",
        "\n",
        "Para entrenar nuestra red usaremos la función de pérdida de entropía cruzada (ver la parte 3a). Dado un conjunto (mini batch) de ejemplos $\\{(x_1,y_1),\\ldots,(x_B,y_B)\\}$, llamemos $x$ al tensor que contiene a todos los $x_i$'s (*apilados* en su dimensión $0$), y similarmente llamemos $y$ al tensor que contiene a todos los $y_i$'s. La función de pérdida de la red se puede entonces escribir como\n",
        "\\begin{equation}\n",
        "\\cal L=\\it{CELoss}(\\hat{y},{y})\n",
        "\\end{equation}\n",
        "donde $\\hat{y}$ es el tensor que se obtiene en la pasada hacia adelante de nuestra red desde $x$ (es decir $\\hat y=\\text{forward}(x)$), y $\\it{CELoss}(\\hat{y},{y})$ es la función de entropía cruzada aplicada a ambos. En esta parte computaremos las derivadas parciales\n",
        "\\begin{equation}\n",
        "\\frac{\\partial \\cal L}{\\partial p}\n",
        "\\end{equation}\n",
        "para cada parámetro $p$ de nuestra red. <br><br>"
      ]
    },
    {
      "metadata": {
        "id": "1BhUhivg37d0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3a) Entropía Cruzada\n",
        "\n",
        "Comenzaremos haciendo una función para computar la pérdida de nuestra red. Recuerda que para dos distribuciones de probabilidad discreta $p(x)$ y $q(x)$ la entropía cruzada (cross entropy) entre $p$ y $q$ está dada por\n",
        "\\begin{equation}\n",
        "\\it{CE}(p,q)=- \\sum_{x}p(x)\\log q(x)\n",
        "\\end{equation}\n",
        "donde $x$ varía sobre todos los posibles valores sobre los cuales la distribución está definida.\n",
        "\n",
        "En esta parte debes programar la función `cross_ent_loss` que recibe tensores $Q_{ij}$ y $P_{ij}$ (de las mismas dimensioens) y calcula el promedio de las entropías cruzadas de las distribuciones $p_i$ y $q_i$ de la siguiente forma\n",
        "\\begin{equation}\n",
        "\\it{CELoss}(Q,P)=\\frac{1}{N}\\sum_{i}\\it{CE}(p_{i}, q_{i})\n",
        "\\end{equation}\n",
        "donde $p_i(x)=P_{ix}$, $q_i(x)=Q_{ix}$ y $N$ es el tamaño de la primera dimension de los tensores (dimension `0`). Nota que el resultado es un escalar."
      ]
    },
    {
      "metadata": {
        "id": "aGaXuo315NDK",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Tu código acá\n",
        "\n",
        "def cross_ent_loss(Q,P):\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SgMS0rHQO9xv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3b) Derivando la última capa\n",
        "\n",
        "Recuerde que la predicción de nuestra red está dada por\n",
        "\\begin{equation}\n",
        "\\hat y = \\text{softmax}(h^{(L)}U+c).\n",
        "\\end{equation}\n",
        "Nuestro objetivo es calcular la derivada de $\\cal L$ con respcto a $U$, $h^{(L)}$ y $c$. Para esto llamemos primero\n",
        "\\begin{equation}\n",
        "u^{(L)} = h^{(L)}U+c.\n",
        "\\end{equation}\n",
        "Nota que con esto, nuestra predicción es simplemente $\\hat y=\\text{softmax}(u^{(L)})$.\n",
        "<br/><br/>\n",
        "Calcula la derivada (el *gradiente*) de $\\cal L$ respecto a $u^{(L)}$, y escribe un trozo de código usando las funcionalidades de `torch` que calcule el valor y lo almacene en una variable `dL_duL`, suponiendo que cuentas con los tensores para $y$ (`y`) e $\\hat y$  (`y_pred`).\n"
      ]
    },
    {
      "metadata": {
        "id": "ngpRWD9XaZ0t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Puedes escribir tu cálculo acá o entregarlo en otro archivo<br/>\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial \\cal L}{\\partial u^{(L)}} =\n",
        "\\end{equation}"
      ]
    },
    {
      "metadata": {
        "id": "jM27Fd-5fqIQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Para ir chequeando que al menos las dimensiones de los tensores son \n",
        "# consistentes usaremos las varibles *dummy* a continuación.\n",
        "\n",
        "B = 5; C = 10\n",
        "\n",
        "y = torch.ones(B,C)\n",
        "y_pred = torch.ones(B,C)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jYNlUtuZb3kM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Acá tu trozo de código. \n",
        "# Primero agregamos algunas variables dummy para chequear \n",
        "# que al menos las dimensiones están correctas\n",
        "\n",
        "dimL = 40\n",
        "\n",
        "hL = torch.ones(B,dimL)\n",
        "U = torch.ones(dimL,C)\n",
        "c = torch.ones(C)\n",
        "\n",
        "uL = hL.mm(U).add(c)\n",
        "\n",
        "# Ahora tu fórmula para el gradiente\n",
        "\n",
        "dL_duL = \n",
        "\n",
        "# El gradiente debe coincidir en dimensiones con la variable\n",
        "\n",
        "assert dL_duL.size() == uL.size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yn8l0EWVa6r_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3b) Derivando la última capa (continuación)\n",
        "\n",
        "Usa la derivada de $\\cal L$ con respecto a $u^{(L)}$ y la regla de la cadena para encontrar las derivadas (*gradientes*) de $\\cal L$ con respecto a $U$, $c$ y $h^{(L)}$. Recuerda tener cuidado con los índices de los tensores, chequear que las dimensiones sean las correctas y cuando sea necesario usa [la notación de Einstein](https://en.wikipedia.org/wiki/Einstein_notation) para simplificar tu vida. Escribe también un trozo de código para calcular estas derivadas y almacenarlas en `dL_dU`, `dL_dc` y `dL_dhL`."
      ]
    },
    {
      "metadata": {
        "id": "LynSkWUvdMx_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Puedes entregar esta parte en otro archivo o puedes escribir tus respuestas acá.\n",
        "<br><br>\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial\\cal L}{\\partial U} = \\ldots \\\\\n",
        "\\end{equation}\n",
        "<br><br>\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial\\cal L}{\\partial c} = \\ldots \\\\\n",
        "\\end{equation}\n",
        "<br><br>\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial\\cal L}{\\partial h^{(L)}} = \\ldots \\\\\n",
        "\\end{equation}\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "qmTHna9EFmtJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Acá tu trozo de código. \n",
        "\n",
        "dL_dU = None\n",
        "\n",
        "dL_dc = None\n",
        "\n",
        "dL_dhL = None\n",
        "\n",
        "# El gradiente debe coincidir en dimensiones con las variables\n",
        "\n",
        "assert dL_dU.size() == U.size()\n",
        "assert dL_dc.size() == c.size()\n",
        "assert dL_dhL.size() == hL.size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gSOzqL9Okh6B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3c) Derivando desde las capas escondidas\n",
        "\n",
        "Ahora derivaremos las capas escondidas en general para todas las funciones de activación que consideramos en esta tarea. **Importante** esta parte es larga no la empieces a hacer tarde!\n",
        "\n",
        "Consideremos la capa $k$, en este caso tenemos\n",
        "\\begin{equation}\n",
        "h^{(k)} = f(h^{(k-1)}W^{(k)}+b^{(k)})\n",
        "\\end{equation}\n",
        "done $f$ es una de las funciones de activación $\\text{sig}, \\text{tanh}, \\text{relu}, \\text{celu}, \\text{swish}$. Lo que queremos es computar las derivadas parciarles de $\\cal L$ con respecto a $W^{(k)}$, $b^{(k)}$ y  $h^{(k-1)}$. Para esto consideremos\n",
        "\\begin{equation}\n",
        "u^{(k)}=h^{(k-1)}W^{(k)}+b^{(k)}.\n",
        "\\end{equation}\n",
        "Supondremos que ya tenemos computado (antes) el gradiente de $\\cal L$ con respecto a $h^{(k)}$ ($\\partial \\cal L/\\partial h^{(k)}$).<br/><br/>\n",
        "\n",
        "\n",
        "Para cada función de activación de entre \n",
        "$\\text{relu}, \\text{celu}, \\text{swish}$, calcula primero \n",
        "\\begin{equation}\n",
        "\\frac{\\partial \\cal L}{\\partial u^{(k)}}\n",
        "\\end{equation}\n",
        "usando $\\partial \\cal L/\\partial h^{(k)}$ y luego usa $\\partial \\cal L/\\partial u^{(k)}$ y la regla de la cadena para calcular\n",
        "\\begin{equation}\n",
        "\\frac{\\partial \\cal L}{\\partial W^{(k)}}, \\frac{\\partial \\cal L}{\\partial b^{(k)}}, \\frac{\\partial \\cal L}{\\partial h^{(k-1)}}. \n",
        "\\end{equation}\n",
        "Crea trozos de código para cada uno de los cálculos de los gradientes.\n"
      ]
    },
    {
      "metadata": {
        "id": "-0AM0h7suDv4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Puedes entregar esta parte en otro archivo o puedes escribir tus respuestas acá.\n",
        "<br><br>\n",
        "\n",
        "Repite los siguientes cálculos para $\\text{relu}, \\text{celu}, \\text{swish}$\n",
        "<br><br>\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial\\cal L}{\\partial u^{(k)}} = \\ldots \\\\\n",
        "\\end{equation}\n",
        "<br><br>\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial\\cal L}{\\partial W^{(k)}} = \\ldots \\\\\n",
        "\\end{equation}\n",
        "<br><br>\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial\\cal L}{\\partial b^{(k)}} = \\ldots \\\\\n",
        "\\end{equation}\n",
        "<br><br>\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial\\cal L}{\\partial h^{(k-1)}} = \\ldots \\\\\n",
        "\\end{equation}"
      ]
    },
    {
      "metadata": {
        "id": "J7OoPdpPsY8x",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Acá tu trozo de código. \n",
        "# Primero agregamos algunas variables dummy para chequear \n",
        "# que al menos las dimensiones están correctas\n",
        "\n",
        "dimk = 20\n",
        "dimkm1 = 30\n",
        "\n",
        "hk = torch.ones(B,dimk)\n",
        "Wk = torch.ones(dimk,dimkm1)\n",
        "bk = torch.ones(dimkm1)\n",
        "\n",
        "uk = hk.mm(Wk).add(bk)\n",
        "\n",
        "dL_dhkm1 = torch.ones(B,dimkm1)\n",
        "\n",
        "# Ahora tu fórmula para el gradiente.\n",
        "# Esto debes repetirlo para relu, celu, y swish\n",
        "\n",
        "dL_duk = None\n",
        "dL_dWk = None\n",
        "dL_dbk = None\n",
        "dL_dhk = None\n",
        "\n",
        "# El gradiente debe coincidir en dimensiones con las variables\n",
        "\n",
        "assert dL_dWk.size() == Wk.size()\n",
        "assert dL_dbk.size() == bk.size()\n",
        "assert dL_dhk.size() == hk.size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GoniZJN_wvEj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Parte 4: Backpropagation en nuestra red\n",
        "\n",
        "En esta parte pondremos todos nuestros cálculos anteriores dentro del método `backward` de nuestra red."
      ]
    },
    {
      "metadata": {
        "id": "otIiI1mNygen",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4a) Método `backward`\n",
        "\n",
        "Programa un método `backward` dentro de la clase FFNN. El método debiera recibir como entrada tres tensores `x`, `y`, `y_pred`, y debiera computar todos los gradientes para cada uno de los parámetros de la red (con todas las suposiciones que hicimos en la Parte 3, incluyendo el uso de entropía cruzada como función de pérdida). Recuerda computar los gradientes también para capas escondidas con activaciones $\\text{sig}$ y $\\text{tanh}$. Deja todos los gradientes computados en una lista `gradientes` que sea un atributo de la clase FFNN."
      ]
    },
    {
      "metadata": {
        "id": "FvcnNQLc0Y5a",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Tu código debiera continuar aquí \n",
        "\n",
        "class FFNN():\n",
        "  def __init__(self, F, l_h, l_a, C):\n",
        "    pass\n",
        "  \n",
        "  def forward(x):\n",
        "    # ya lo creaste en la parte anterior\n",
        "    pass\n",
        "  \n",
        "  def backward(x,y,y_pred):\n",
        "    # computar acá todos los gradientes\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7KdiE3RDwERd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4b) Checkeo de gradiente\n",
        "\n",
        "Programa un método `chequea_gradiente` que reciba un objeto FFNN y que implemente el método de chequeo numérico de gradiente (como se explicó en clases). La idea es que se pueda llamar a este método después de `forward` y `backward` para verificar que no hay errores en la computación de las derivadas parciales. El uso debiera ser como el siguiente:\n",
        "\n",
        "``` \n",
        "# x, y son tensores con datos de entrada\n",
        "# red es un objeto FFNN previamente inicializado\n",
        "\n",
        "y_pred = red.forward(x)\n",
        "red.backward(x,y,y_pred)\n",
        "if not chequa_gradiente(red):\n",
        "  print('Algo malo pasó :-(')\n",
        "else:\n",
        "  print('Todo bien con los gradientes!')\n",
        "``` \n",
        "\n",
        "Puede ser bueno generar algún tipo de mensaje acerca del parámetro (o los parámetros) para los cuales el chequeo falló.\n",
        "\n",
        "Luego de programada la función, muestra dos escenarios para tu red, uno en donde compruebes que los gradientes quedaron bien computados y otro en donde la función encuentre un error. (Para el último caso puedes simplemente forzar el error cambiando a mano algún valor de uno de los parámetros o uno de los gradientes.)\n",
        "\n",
        "\n",
        "Nota que toda tu tarea puede funcionar sin programar esta parte, pero hacer el chequeo de gradiente de forma correcta puede evitarte muchísimos dolores de cabeza para debuggear, porque te ayudará a verificar que las derivadas que calculaste a mano funcionan como se espera."
      ]
    },
    {
      "metadata": {
        "id": "7zcMfV-XwMb9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Tu código de chequeo de gradiente acá"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cjFVG5e9LlD6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4c) Opcional: Incluyendo los parámetros de `celu` y `swish`\n",
        "\n",
        "Si lo deseas, puedes agregar los parámetros de todas las activaciones `celu` y `swish` de tu red como parámetros entrenables. Para esto tendrás que agregar parámetros en el inicializador de la clase y computar las derivadas correspondientes en la función `backward`."
      ]
    },
    {
      "metadata": {
        "id": "AE3EveEj2JEP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Parte 5: Descenso de gradiente y entrenamiento (por fin!)\n",
        "\n",
        "En esta parte programaremos el algoritmo de descenso de gradiente más común y entrenaremos finalmente nuestra red para que aprenda a clasificar datos aleatorios y un conjunto muy simple de datos (aunque también puedes usar datos de verdad para entrenarla si es que te interesa)."
      ]
    },
    {
      "metadata": {
        "id": "oG-I7Oyl-qsd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5a) Descenso de gradiente (estocástico)\n",
        "\n",
        "Construye una clase `SGD` que implemente el algoritmo de descenso de gradiente. El inicializador de la clase debe recibir al menos dos parámetros: una red neuronal (objeto FFNN), y una taza de aprendizaje (`lr`), e implementar un único método `step` que debe actualizar todos los parámetros de la red para los que ya se han computado los gradientes. El uso de esta clase debiera ser como  sigue:\n",
        "\n",
        "\n",
        "``` \n",
        "# datos es un iterador sobre pares de tensores x, y\n",
        "# red es un objeto FFNN previamente inicializado\n",
        "\n",
        "optimizador = SGD(red, 0.001)\n",
        "for x,y in datos:\n",
        "  y_pred = red.forward(x)\n",
        "  red.backward(x,y,y_pred)\n",
        "  optimizador.step()\n",
        "``` \n"
      ]
    },
    {
      "metadata": {
        "id": "TUkJA9Qm2GKY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Tu código debiera comenzar así\n",
        "\n",
        "class SGD():\n",
        "  def __init__(self, red, lr):\n",
        "    # lo que sea necesario inicializar\n",
        "    pass\n",
        "  \n",
        "  def step():\n",
        "    # actualiza acá los parámetros a partir de los gradientes\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vpCfOvvbLdoq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5b) Datos para carga\n",
        "\n",
        "En esta parte crearás un conjunto de datos de prueba aleatorios para probar con tu red. Se recomienda partir por datos al azar, simplemente para que no haya significado en ellos  y te puedas concentrar en encontrar posibles bugs en tu implementación. En la Parte 5e, usarás un conjunto simple de entrenamiento para que veas a tu red aprendiendo sobre conjuntos de datos reales.\n",
        "\n",
        "Para esta parte debes crear una clase `RandomDataset` que recibirá en su inicializador la cantidad de ejemplos a crear, la cantidad de features de cada ejemplo, y la cantidad de clases en la función objetivo. Tu clase debe además definir la función `__len__` que retorna el largo del dataset y la función `__getitem__` que permite acceder a un item específico de los datos. Adicionalmente debes definir una función `paquetes` que recibe un parámetro `B` y entrega un iterador que permita pasar por todos los ejemplos del dataset en paquetes de tamaño `B`. El iterador debiera entregar pares de tensores `(x,y)` tal que el tensor `x` es de dimensiones `(B,F)` y el tensor `y` es de dimensiones `(B,C)`. Nota que la gracia del iterador es que puedas hacer un loop como el que sigue:\n",
        "\n",
        "```\n",
        "dataset = RandomDataset(100,300,10)\n",
        "for x,y in dataset.paquetes(4):\n",
        "  # x,y son paquetes de 4 ejemplos para la red.\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "U6o4TjFZa3gJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Aquí tu código.\n",
        "# Tu clase debiera verse así\n",
        "\n",
        "class RandomDataset():\n",
        "  def __init__(self, N, F, C):\n",
        "    pass\n",
        "  \n",
        "  def __len__(self):\n",
        "    pass\n",
        "  \n",
        "  def __getitem__(self, i):\n",
        "    pass\n",
        "  \n",
        "  def paquetes(self, B):\n",
        "    pass\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bm4X2oazLifX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5c) Entrenando la red\n",
        "\n",
        "Por fin podrás crear un loop de entrenamiento. Para esto crea la función `entrenar_FFNN` que recibe una red, un dataset, un optimizador, la cantidad de epochs por las que se quiere entrenar y un valor `B` que indica el tamaño de los paquetes de ejemplos usados en el entrenamiento. Puedes definir todos los parámetros adicionales que quieras para la función. \n",
        "\n",
        "Dentro de la función debes hacer tantas iteraciones sobre el dataset como sea indicado por el parámetro epochs utilizando el optimizador entregado. Al finalizar la función debe retornar la red entrenada y una lista con el valor de la pérdida en cada iteración. Asegúrate de que la función muestra información relevante durante el entrenamiento (piensa en toda la información que te gustaría tener mientras la red entrena e imprímela!).\n",
        "\n",
        "El uso de la función, y de todo tu código hasta ahora, debiera ser como sigue:\n",
        "\n",
        "``` \n",
        "F = 300; C = 10; N = 100\n",
        "red = FFNN(F, [50,30], [relu,sig], C)\n",
        "optimizador = SGD(red, 0.001)\n",
        "dataset = RandomDataset(N, F, C)\n",
        "red_entrenada, perdida = entrenar_FFNN(red, dataset, optimizador, 20, 4)\n",
        "``` "
      ]
    },
    {
      "metadata": {
        "id": "s4jgU-fDUPUD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Tu código acá\n",
        "\n",
        "def entrenar_FFNN(red, dataset, optimizador, epochs, B):\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ptr16D9gYAC6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5d) Graficando la pérdida en el tiempo\n",
        "\n",
        "Usa la librería [`matplotlib`](https://matplotlib.org/users/index.html) para mostrar cómo varía la pérdida a medida que entrena tu red. Muestra al menos tres redes distintas entrenadas con el mismo conjunto."
      ]
    },
    {
      "metadata": {
        "id": "sZGAxnR-Ll-Y",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Tu código acá"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hWkGbQPHmaFN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5e) Entrenando con datos no random\n",
        "\n",
        "Usa tu red para resolver el problema \"Varita Mágica\" de la parte 2d) (los datos y una descripción del problema los puedes encontrar [acá](https://github.com/jorgeperezrojas/cc6204-DeepLearning-DCCUChile/tree/master/2018/tareas/tarea1/recursos/varita_magica)). El conjunto de datos es sumamente simple, así que no deberías necesitar una red muy grande para resolver el problema. Esto te servirá más que nada para ver que tu red efectivamente es capaz de aprender datos. Haz primero una clase que te permita pasar por los datos de entrenamiento (similar a `RandomDataset`).\n",
        "\n",
        "Prueba un par de configuraciones para tu red y reporta el porcentaje de acierto (cantidad de casos clasificados correctamente) en el conjunto de prueba. Deberías obtener más que el porcentaje de certeza que obtenía el modelo de ejemplo de la parte 2d).\n",
        "\n",
        "Y si te animas puedes también pasarle inputs creados por ti (fuera del set de entrenamiento y prueba provistos).\n",
        "\n",
        "(Después de esto puedes pasar directamente a la Parte 8 si quieres enfrentarte ya a un conjunto de entrenamiento y prueba más complejo.)"
      ]
    },
    {
      "metadata": {
        "id": "7PJ_w-4UMeU9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Tu código de carga de datos, creación de la red, \n",
        "# entrenamiento y reportes acá"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y03bJGaDGuII",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Parte 6: Regularización (en construcción)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "pnA_CbikHNHv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Parte 7: Optimización (en construcción)"
      ]
    },
    {
      "metadata": {
        "id": "VTjXiZcTHZgm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Parte 8: Entrenando sobre MNIST (en construcción)\n",
        "\n",
        "En esta parte pondrás a prueba todo lo que has construído para tratar de entrenar una buena red que clasifique dígitos escritos a mano. Usaremos el conjunto de datos MNIST cuya versión original junto con una descripción del conjunto y resultados para distintos métodos de clasificación, se pueden encontrar en http://yann.lecun.com/exdb/mnist/"
      ]
    },
    {
      "metadata": {
        "id": "V4YdUVgQIUWK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 8a) Cargando y visualizando datos de MNIST\n",
        "\n",
        "Esta parte no requiere que escribas código, sólo que te familiarices con el conjunto de datos. Sólo sigue las instrucciones.\n",
        "\n",
        "Primero usaremos el paquete `torch.vision` para descargar y procesar los datos de MNIST."
      ]
    },
    {
      "metadata": {
        "id": "YE1iMUNfIT1r",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        " # Importamos las clases necesarias\n",
        "\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from matplotlib.pyplot import imshow, figure, subplots"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vHFd_ttpGjZ4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {}
          ],
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "a31db182-b2a3-40f2-82f8-7ee756bbd7f1",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520907431781,
          "user_tz": 180,
          "elapsed": 23739,
          "user": {
            "displayName": "Jorge Perez",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107068936185859088499"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Descarga y almacena el conjunto de entrenamiento y prueba de MNIST\n",
        "# Además aplica una transformación para convertir todas las imágenes a \n",
        "# tensores de pytroch\n",
        "\n",
        "train_data = MNIST('mnist', train=True, download=True, transform=ToTensor())\n",
        "test_data = MNIST('mnist', train=False, transform=ToTensor())\n",
        "\n",
        "print('Cantidad de ejemplos de entrenamiento: ' + str(len(train_data)))\n",
        "print('Cantidad de ejemplos de prueba: ' + str(len(test_data)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n",
            "Cantidad de ejemplos de entrenamiento: 60000\n",
            "Cantidad de ejemplos de prueba: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FLEHQsTyJsgF",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {}
          ],
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "outputId": "c1bc99af-3072-47df-a474-699e0b455f3c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520907576301,
          "user_tz": 180,
          "elapsed": 1170,
          "user": {
            "displayName": "Jorge Perez",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107068936185859088499"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Muestra 5 ejemplos al azar usando un DataLoader\n",
        "\n",
        "dataloader = DataLoader(train_data, shuffle=True)\n",
        "n_ejemplos = 5\n",
        "\n",
        "fig, axs = subplots(nrows=n_ejemplos, sharey=True, figsize=(3,n_ejemplos*3))\n",
        "\n",
        "for i, batch in enumerate(dataloader):\n",
        "  if i == n_ejemplos:\n",
        "    break\n",
        "    \n",
        "  img, d = batch\n",
        "  axs[i].set_title(\"target: \" + str(d.numpy()))\n",
        "  axs[i].imshow(img.view(28,28).numpy())\n",
        "\n",
        "# Note que se usó `view` para redimensionar el tensor, esto porque nuestro\n",
        "# dataloader entrega un tensor de dimensiones (1,1,28,28).\n",
        "# Es muy importante tener este hecho en cuenta en la siguiente parte."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMwAAANdCAYAAADY8JOJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRBL+Bw5WM3Lcq0hEAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f6b72639210>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ob9VdT3NKEBP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 8b) Red neuronal para MNIST\n",
        "\n",
        "Usa tu red neuronal (o modifícala como quieras) para entrenar la mejor red que puedas para MNIST. Puedes considerar y variar todo lo que estimes como cantidad de capas, método de regularización, mejores optimizaciones, o incluso puedes implementar nuevas técnicas mejorando tu código anterior. Lo importante es que las técnicas que pruebes debes implementarlas tu. Recuerda que **solo puedes usar el conjunto de entrenamiento para entrenar a la red. Buscar los hiperparámetros, o cualquier otra cosa, en el conjunto de test es trampa**.\n",
        "\n",
        "Reporta acá qué hiciste para encontrar la mejor configuración de red, cuál fue la red final escogida, y reporta también los valores de los mejores hiperparámetros que encontraste para el entrenamiento. Tu experimento debe ser repetible (o sea, otra persona debe ser capaz de correr el mismo experimento con tu código y obtener los mismos resultados). Ten cuidado que dependiendo de la configuración de tu red, esta puede demorarse un tiempo considerable en entrenar, así que considéralo! Comienza con una red muy simple (una capa escondida) para que la tengas como punto de comparación antes de ponerte a probar con alguna otra muy compleja.\n",
        "\n",
        "Reporta también al menos un gráfico del entrenamiento, en particular como varió la pérdida durante el entrenamiento. Reporta finalmente la certeza de las predicciones de tu red (porcentaje de ejemplos que clasifica correctamente)."
      ]
    }
  ]
}