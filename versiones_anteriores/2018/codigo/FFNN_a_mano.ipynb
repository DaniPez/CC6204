{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FFNN_a_mano_v0.1","version":"0.3.2","views":{},"default_view":{},"provenance":[{"file_id":"1JVNfrxNN8jpwjOszpwEegnKzvwDhGYWF","timestamp":1520378988084}],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"rr5EsNZxJhN8","colab_type":"text"},"cell_type":"markdown","source":["# CC6204 Deep Learning, Universidad de Chile\n","## Código de Red Neuronal \"a mano\"\n","El siguiente código muestra de manera tan directa como fue posible la forma de crear una red neuronal Feed Forward de dos capas escondidas usando solo las funcionalidades para operar tensores de pytorch (solo clases y funciones dentro del módulo [`torch`](http://pytorch.org/docs/master/torch.html#module-torch)).\n","\n","La idea del código es mostrar cómo:\n","\n","*   crear todos los parámetros de una red Feed Forward usando tensores,\n","*   computar la pasada hacia adelante de la red (predicción/forward) usando funciones sobre los mismos tensores y los datos de entrada por paquetes/batches (usando funciones de sigmoid y tangente hiperbólica como activación, y sigmoid final para el outpu),\n","*   computar la función de pérdida para la red (entropía cruzada en este caso),\n","*   calcular los gradientes desde la función de pérdida para todos los parámetros usando operadores sobre los tensores con el método de Back Propagation (backward), \n","*   actualizar los parámetros usando descenso de gradiente, y\n","*   reportar las métricas de predicción de la red sobre los datos.\n","\n","Todos los pasos anteriores se repiten en un loop de entrenamiento por tantas iteraciones como se quiera (epochs). Otro punto muy importante es que gracias a pytorch y CoLaboratory, podemos realizar todas las pruebas de manera muy simple en una GPU. El código también sirve para explorar el impacto de realizar estos entrenamientos con hardware especializado.\n","\n","Una de las gracias de usar pytorch es que nos permite hacer todo lo anterior a mano, paso a paso, sólo utilizando funciones básicas. Esto se podría también haber realizado utilizando sólo [NumPy](http://www.numpy.org/)  (es un buen ejercicio, hágalo!), pero no podríamos usar la GPU tan fácilmente como en pytorch.\n","\n","El código está pensado para acompañar la clase de Back Propagation de CC6204 y servir como una introducción rápida a las funcionalidades básicas de pytorch. No está pensado en ser un código modular, si no más bien un código pedagógico para los temas de grafo de computación, back propagation, y descenso de gradiente por paquetes. Para entender el código requiere haber calculado antes derivadas de la función de pérdida (con lapiz y papel).\n","\n","(Pensado para correr en [Colaboratory](http://colab.research.google.com))\n","\n"]},{"metadata":{"id":"solFJy_c2iJN","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# http://pytorch.org/\n","from os import path\n","from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","\n","accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n","\n","!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n","!pip install -q ipdb"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2JVQ13l32QpB","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import torch\n","import numpy as np\n","import sys\n","import time\n","import ipdb\n","\n","# Genera una semilla fija para que los experimentos sea repetibles.\n","t_cg = torch.manual_seed(1547)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hDMZskhE2d1C","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Chequeamos si hay acceso a la GPU.\n","print(torch.cuda.is_available())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"U_EdIfcNA49D","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Funciones de activación:\n","\n","# Función sigmoid, recibe un objeto torch.Tensor\n","def sig(T):\n","  return torch.reciprocal(1 + torch.exp(-1 * T))\n","\n","# Función tanh, recibe un objeto torch.Tensor\n","def tanh(T):\n","  E = torch.exp(T)\n","  e = torch.exp(-1 * T)\n","  return (E - e) * torch.reciprocal(E + e)\n","\n","# Función de pérdida\n","def bi_cross_ent_loss(y_pred, y, safe=True, epsilon=1e-5):\n","  N = y.size()[0]\n","  \n","  if safe:\n","    # Asegura que no haya valores indefinidos.\n","    y_pred.clamp_(epsilon, 1-epsilon)\n","  \n","  B = (1-y) * torch.log(1-y_pred) + y * torch.log(y_pred)\n","  return -1/N * torch.sum(B)  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"SDWK7JHTbw7n","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Para elegir el siguiente batch (uno al azar) desde los datos de entrada\n","def elige_batch(X,Y,b):\n","  N = X.size()[0]\n","  x_lista = []\n","  y_lista = []\n","  \n","#  i = np.random.randint(N-b) # <-- descomentar esto para ejemplos\n","  for _ in range(b):\n","    i = np.random.randint(N) # <-- comentar esto para ejemplos\n","    x_lista.append(X[i:i+1])\n","    y_lista.append(Y[i:i+1])\n","  \n","  x = torch.cat(x_lista, dim=0)\n","  y = torch.cat(y_lista, dim=0)\n","  \n","  return x,y"],"execution_count":0,"outputs":[]},{"metadata":{"id":"I-nerXbn3X4z","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def ejemplo_FFNN(X, Y, b=1, d1=200, d2=300, lr=0.06, \n","                 epochs=10, run_in_GPU=True, reports_every=1, \n","                 cheq_grad=False, init_v=1):\n","  \n","  # Define un tipo para los tensores según si correrá en la GPU o no\n","  if run_in_GPU:\n","    assert torch.cuda.is_available()\n","    t_type = torch.cuda.FloatTensor\n","  else:\n","    t_type = torch.FloatTensor\n","    \n","  # Numero de ejemplos y cantidad de features\n","  N = X.size()[0]\n","  f = X.size()[1]\n","  \n","  # d0 es la cantidad de features  \n","  d0 = f\n","  \n","  # Crea los tensores de parámetros\n","  W1 = torch.randn(d0,d1).type(t_type) * init_v\n","  b1 = torch.zeros(d1).type(t_type)\n","  W2 = torch.randn(d1,d2).type(t_type) * init_v\n","  b2 = torch.zeros(d2).type(t_type)\n","  U = torch.randn(d2,1).type(t_type) * init_v\n","  c = torch.randn(1).type(t_type) \n","  \n","  parametros = {'W1':W1, 'b1':b1, 'W2':W2, 'b2':b2, 'U':U, 'c':c}\n","  \n","  # Cuenta los parámetros en total\n","  cant_parametros = 0\n","  for P in parametros:\n","    cant_parametros += parametros[P].nelement()      \n","  print('Cantidad de parámetros:', cant_parametros)\n","     \n","  tiempo_epochs = 0\n","  for e in range(1,epochs+1):  \n","    inicio_epoch = time.clock()\n","    # Cantidad de iteraciones por epoch (b es el tamaño del batch)\n","    I = int(N/b) \n","    \n","    for i in range(I):\n","      x, y = elige_batch(X,Y,b)\n","      # Asegura de pasarlos a la GPU si fuera necesario\n","      x = x.type(t_type)\n","      y = y.type(t_type)\n","      \n","      # Computa la pasada hacia adelante (forward)\n","      u1 = x.mm(W1).add(b1)\n","      h1 = tanh(u1)\n","      u2 = h1.mm(W2).add(b2)\n","      h2 = sig(u2)\n","      u3 = h2.mm(U).add(c)\n","      y_pred = sig(u3)\n","                  \n","      # Computa la función de pérdida\n","      L = bi_cross_ent_loss(y_pred,y)\n","      \n","      # Computa los gradientes hacia atrás (backpropagation)\n","      # Estas son derivadas calculadas a mano\n","      dL_du3 = (1/b) * (y_pred - y)\n","      dL_dU  = torch.t(h2).mm(dL_du3)\n","      dL_dc  = torch.sum(dL_du3,0)\n","      dL_dh2 = dL_du3.mm(torch.t(U))\n","      dL_du2 = dL_dh2 * sig(u2) * (1 - sig(u2)) \n","      dL_dW2 = torch.t(h1).mm(dL_du2)\n","      dL_db2 = torch.sum(dL_du2,0)\n","      dL_dh1 = dL_du2.mm(torch.t(W2))\n","      dL_du1 = dL_dh1 * (1 - tanh(u1) * tanh(u1))\n","      dL_dW1 = torch.t(x).mm(dL_du1)\n","      dL_db1 = torch.sum(dL_du1,0)\n","            \n","      gradientes = {'c': dL_dc, 'U':dL_dU, 'W2':dL_dW2, 'b2':dL_db2, \n","                    'W1':dL_dW1, 'b1':dL_db1}\n","      \n","      # Actualiza los pesos\n","      for P in parametros:\n","        parametros[P] -= lr * gradientes[P]\n","      \n","    tiempo_epochs += time.clock() - inicio_epoch\n","    \n","    if e % reports_every == 0:\n","      # Calcula la certeza de las predicciones sobre todo el conjunto\n","      X = X.type(t_type) # pasa a la GPU si fuera necesario\n","      Y = Y.type(t_type) # pasa a la GPU si fuera necesario\n","\n","      # Predice usando la red\n","      H1 = tanh(X.mm(W1).add(b1))\n","      H2 = sig(H1.mm(W2).add(b2))\n","      Y_PRED = sig(H2.mm(U).add(c))\n","      \n","      # Calcula la pérdida de todo el conjunto\n","      L_total = bi_cross_ent_loss(Y_PRED, Y, safe=True)\n","\n","      # Elige una clase dependiendo del valor de Y_PRED\n","      Y_PRED_BIN = (Y_PRED >= 0.5).float()\n","\n","      correctos = torch.sum(Y_PRED_BIN == Y)\n","      acc = correctos / N * 100\n","\n","      sys.stdout.write(\n","            '\\rEpoch:{0:03d}'.format(e) + ' Acc:{0:.2f}%'.format(acc)\n","            + ' Loss:{0:.4f}'.format(L_total) \n","            + ' Tiempo/epoch:{0:.3f}s'.format(tiempo_epochs/e)) \n","  \n","  return parametros"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uBBSkuxaEI7z","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["N = 5000 # numero de ejemplos\n","f = 300 # numero de features\n","\n","\n","X = torch.rand(N,f)\n","X = torch.bernoulli(X)\n","\n","Y = torch.rand(N,1)\n","Y = torch.bernoulli(Y)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XgaASdM0FnJ9","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["epochs = 30\n","\n","red = ejemplo_FFNN(X, Y, b=32, d1=300, d2=400, epochs=epochs, \n","             run_in_GPU=True, lr=0.06, init_v=0.8)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Oluz-gTqF09b","colab_type":"text"},"cell_type":"markdown","source":["Para ejemplificar, probar el anterior código de la siguiente forma:\n","\n","*   Cambiar el tamaño del batch desde 2 a 1000: visualizar tiempo de entrenamiento vs Acc\n","*   Cambiar el tamaño de las capas / número de parámetros: visualizar tiempo de entrenamiento\n","*   Cambiar el valor máximo de inicialización (0.01,1,1.5,2)\n","*   Mostrar que se pueden calcular (aun) más eficiente la pasada hacia atrás reutilizando algunos valores previamente computados cuando derivamos `sig` y `tanh`: visualizar el tiempo de entrenamiento.\n","*   Descomentar la línea del generador de batches. ¿Cómo se explica el resultado?\n"]},{"metadata":{"id":"0yXpoazIcMp6","colab_type":"text"},"cell_type":"markdown","source":["Código por Jorge Pérez\n","\n","https://github.com/jorgeperezrojas\n","\n","@perez"]}]}